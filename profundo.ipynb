{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from app import MLP\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, layer_sizes, activations, cost_fn, optimizer, init_method, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.activations = activations\n",
    "        self.cost_fn = cost_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.init_method = init_method\n",
    "        self.beta1 = beta1  # Adam\n",
    "        self.beta2 = beta2  # Adam\n",
    "        self.epsilon = epsilon  # Adam\n",
    "        self.weights, self.biases = self.initialize_weights()\n",
    "        self.m = 0  # number of samples, initialized during training\n",
    "\n",
    "        if optimizer == 'adam':\n",
    "            self.m_weights = [np.zeros_like(w) for w in self.weights]\n",
    "            self.v_weights = [np.zeros_like(w) for w in self.weights]\n",
    "            self.m_biases = [np.zeros_like(b) for b in self.biases]\n",
    "            self.v_biases = [np.zeros_like(b) for b in self.biases]\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        for i in range(1, len(self.layer_sizes)):\n",
    "            if self.init_method == 'xavier':\n",
    "                w = np.random.randn(self.layer_sizes[i], self.layer_sizes[i-1]) * np.sqrt(1 / self.layer_sizes[i-1])\n",
    "            elif self.init_method == 'he':\n",
    "                w = np.random.randn(self.layer_sizes[i], self.layer_sizes[i-1]) * np.sqrt(2 / self.layer_sizes[i-1])\n",
    "            else:  # normal or uniform\n",
    "                w = np.random.randn(self.layer_sizes[i], self.layer_sizes[i-1])\n",
    "            b = np.zeros((self.layer_sizes[i], 1))\n",
    "            weights.append(w)\n",
    "            biases.append(b)\n",
    "        return weights, biases\n",
    "    \n",
    "    def forward(self, X):\n",
    "        activations = [X]\n",
    "        zs = []\n",
    "        for i, (w, b) in enumerate(zip(self.weights, self.biases)):\n",
    "            # Ensure b is broadcasted correctly\n",
    "            z = np.dot(w, activations[-1]) + b  # z = W * a + b\n",
    "            zs.append(z)\n",
    "            if self.activations[i] == 'relu':\n",
    "                activations.append(self.relu(z))\n",
    "            elif self.activations[i] == 'sigmoid':\n",
    "                activations.append(self.sigmoid(z))\n",
    "            elif self.activations[i] == 'tanh':\n",
    "                activations.append(self.tanh(z))\n",
    "            elif self.activations[i] == 'leaky_relu':\n",
    "                activations.append(self.leaky_relu(z))\n",
    "            else:  # Linear activation\n",
    "                activations.append(z)\n",
    "        return activations, zs\n",
    "    \n",
    "    def backward(self, activations, zs, y):\n",
    "        weight_grads = [None] * len(self.weights)\n",
    "        bias_grads = [None] * len(self.biases)\n",
    "        \n",
    "        if self.cost_fn == 'bce':\n",
    "            dz = activations[-1] - y  # for binary cross-entropy\n",
    "        elif self.cost_fn == 'mse':\n",
    "            dz = (activations[-1] - y) * self.deriv_mse(activations[-1], y)\n",
    "        \n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            # dz is (units in current layer, number of samples)\n",
    "            # activations[i] is (units in previous layer, number of samples)\n",
    "            weight_grads[i] = np.dot(dz, activations[i].T) / self.m  # Ensure proper matrix multiplication\n",
    "            bias_grads[i] = np.sum(dz, axis=1, keepdims=True) / self.m  # Sum over the sample dimension\n",
    "            \n",
    "            if i > 0:\n",
    "                dz = np.dot(self.weights[i].T, dz) * self.deriv_activation(zs[i-1], self.activations[i-1])\n",
    "        \n",
    "        return weight_grads, bias_grads\n",
    "    \n",
    "    def train(self, X, y, epochs, batch_size, learning_rate, X_val, y_val):\n",
    "        self.m = X.shape[1]  # number of samples\n",
    "\n",
    "        # Initialize a list to store loss values and validation accuracy\n",
    "        loss_history = []\n",
    "        acc_history = []\n",
    "\n",
    "        # Set up the plot outside the loop with two subplots\n",
    "        plt.ion()  # Turn on interactive mode for live updating plots\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(21, 6))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            activations, zs = self.forward(X)\n",
    "            loss = self.compute_cost(activations[-1], y)\n",
    "            loss_history.append(loss)  # Append current loss to history\n",
    "            weight_grads, bias_grads = self.backward(activations, zs, y)\n",
    "            acc = self.calculate_acc(X_val, y_val)\n",
    "            acc_history.append(acc)\n",
    "\n",
    "            self.update_parameters(weight_grads, bias_grads, learning_rate, epoch)\n",
    "            if epoch % 10 == 0:\n",
    "                # print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "                self.update_plots(ax1, ax2, ax3, epoch, loss_history, acc_history, epochs)  # Update both plots\n",
    "\n",
    "        # Update plots one final time after training\n",
    "        self.update_plots(ax1, ax2, ax3, epoch, loss_history, acc_history, epochs)  # Update both plots\n",
    "\n",
    "        plt.ioff()  # Turn off interactive mode\n",
    "        plt.show()  # Show the final plot\n",
    "\n",
    "    def update_plots(self, ax1, ax2, ax3, epoch, loss_history, acc_history, total_epochs):\n",
    "        # Clear the previous plots\n",
    "        ax1.clear()\n",
    "        ax2.clear()\n",
    "        ax3.clear()\n",
    "\n",
    "        # --- Update Weight Histogram on ax1 ---\n",
    "        # Combine all layer weights into a single array for the histogram\n",
    "        all_weights = np.concatenate([w.flatten() for w in self.weights])\n",
    "\n",
    "        # Plot histogram of all weights combined\n",
    "        ax1.hist(all_weights, bins=100, color='blue', alpha=0.7)\n",
    "\n",
    "        # Add title and labels\n",
    "        ax1.set_title(f\"Weight Distribution at Epoch {epoch}\")\n",
    "        ax1.set_xlabel(\"Weight values\")\n",
    "        ax1.set_ylabel(\"Frequency\")\n",
    "\n",
    "        # --- Plot Loss over Epochs on ax2 ---\n",
    "        ax2.plot(loss_history, color='red')\n",
    "\n",
    "        # Add title and labels\n",
    "        ax2.set_title(\"Loss over Epochs\")\n",
    "        ax2.set_xlabel(\"Epoch\")\n",
    "        ax2.set_ylabel(\"Loss\")\n",
    "\n",
    "        ax2.set_xlim(0, total_epochs)  # Set x-axis limits to number of epochs\n",
    "\n",
    "        # --- Plot accuracy over Epochs on ax2 ---\n",
    "        ax3.plot(acc_history, color='red')\n",
    "\n",
    "        # Add title and labels\n",
    "        ax3.set_title(\"Accuracy over Epochs in the Validation Data\")\n",
    "        ax3.set_xlabel(\"Epoch\")\n",
    "        ax3.set_ylabel(\"Accuracy\")\n",
    "\n",
    "        ax3.set_xlim(0, total_epochs)  # Set x-axis limits to number of epochs\n",
    "\n",
    "        # Adjust layout to prevent overlap\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Draw the updated plots\n",
    "        fig = ax1.figure  # Get the figure object associated with the axes\n",
    "        fig.canvas.draw()\n",
    "        fig.canvas.flush_events()\n",
    "\n",
    "        # Optional: Pause to allow the plot to update\n",
    "        plt.pause(0.1)\n",
    "\n",
    "    def calculate_acc(self, X_val, y_val):\n",
    "\n",
    "        activations, _ = self.forward(X_val)\n",
    "        y_pred = activations[-1]\n",
    "\n",
    "        # Convert predictions to class labels\n",
    "        y_pred_labels = np.argmax(y_pred, axis=0)\n",
    "        y_true_labels = np.argmax(y_val, axis=0)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        acc = np.mean(y_pred_labels == y_true_labels)\n",
    "        return acc\n",
    "\n",
    "    def update_parameters(self, weight_grads, bias_grads, learning_rate, t):\n",
    "        if self.optimizer == 'sgd':\n",
    "            for i in range(len(self.weights)):\n",
    "                self.weights[i] -= learning_rate * weight_grads[i]\n",
    "                self.biases[i] -= learning_rate * bias_grads[i]\n",
    "        elif self.optimizer == 'adam':\n",
    "            for i in range(len(self.weights)):\n",
    "                # Adam updates\n",
    "                self.m_weights[i] = self.beta1 * self.m_weights[i] + (1 - self.beta1) * weight_grads[i]\n",
    "                self.v_weights[i] = self.beta2 * self.v_weights[i] + (1 - self.beta2) * (weight_grads[i] ** 2)\n",
    "\n",
    "                m_hat_w = self.m_weights[i] / (1 - self.beta1 ** (t + 1))\n",
    "                v_hat_w = self.v_weights[i] / (1 - self.beta2 ** (t + 1))\n",
    "\n",
    "                self.weights[i] -= learning_rate * m_hat_w / (np.sqrt(v_hat_w) + self.epsilon)\n",
    "\n",
    "                self.m_biases[i] = self.beta1 * self.m_biases[i] + (1 - self.beta1) * bias_grads[i]\n",
    "                self.v_biases[i] = self.beta2 * self.v_biases[i] + (1 - self.beta2) * (bias_grads[i] ** 2)\n",
    "\n",
    "                m_hat_b = self.m_biases[i] / (1 - self.beta1 ** (t + 1))\n",
    "                v_hat_b = self.v_biases[i] / (1 - self.beta2 ** (t + 1))\n",
    "\n",
    "                self.biases[i] -= learning_rate * m_hat_b / (np.sqrt(v_hat_b) + self.epsilon)\n",
    "\n",
    "    # Activation functions and their derivatives\n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def tanh(self, z):\n",
    "        return np.tanh(z)\n",
    "    \n",
    "    def leaky_relu(self, z, alpha=0.01):\n",
    "        return np.where(z > 0, z, alpha * z)\n",
    "    \n",
    "    def deriv_activation(self, z, activation_fn):\n",
    "        if activation_fn == 'relu':\n",
    "            return np.where(z > 0, 1, 0)\n",
    "        elif activation_fn == 'sigmoid':\n",
    "            a = self.sigmoid(z)\n",
    "            return a * (1 - a)\n",
    "        elif activation_fn == 'tanh':\n",
    "            return 1 - np.tanh(z)**2\n",
    "        elif activation_fn == 'leaky_relu':\n",
    "            return np.where(z > 0, 1, 0.01)\n",
    "        else:\n",
    "            return 1\n",
    "        \n",
    "        # Add softmax function\n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))  # Subtract max for numerical stability\n",
    "        return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n",
    "    \n",
    "    # Cost functions\n",
    "    def binary_cross_entropy(self, y_pred, y_true):\n",
    "        epsilon = 1e-12  # Small value to avoid log(0)\n",
    "        y_pred = np.clip(y_pred, epsilon, 1. - epsilon)  # Clip predictions to prevent log(0)\n",
    "        m = y_true.shape[1]\n",
    "        cost = -np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)) / m\n",
    "        return np.squeeze(cost)\n",
    "    \n",
    "    def mean_squared_error(self, y_pred, y_true):\n",
    "        # Ensure y_pred and y_true are numpy arrays (not numpy matrices)\n",
    "        y_pred = np.asarray(y_pred)\n",
    "        y_true = np.asarray(y_true)\n",
    "        \n",
    "        return np.mean((y_pred - y_true) ** 2)\n",
    "    \n",
    "    def compute_cost(self, y_pred, y_true):\n",
    "        if self.cost_fn == 'bce':\n",
    "            return self.binary_cross_entropy(y_pred, y_true)\n",
    "        elif self.cost_fn == 'mse':\n",
    "            return self.mean_squared_error(y_pred, y_true)\n",
    "    \n",
    "    def deriv_mse(self, y_pred, y_true):\n",
    "        return 2 * (y_pred - y_true)\n",
    "    \n",
    "    # Model export and import\n",
    "    def export_model(self, filename):\n",
    "        model_data = {\n",
    "            'weights': self.weights,\n",
    "            'biases': self.biases,\n",
    "            'layer_sizes': self.layer_sizes,\n",
    "            'activations': self.activations,\n",
    "            'cost_fn': self.cost_fn,\n",
    "            'optimizer': self.optimizer,\n",
    "            'init_method': self.init_method\n",
    "        }\n",
    "        np.savez(filename, **model_data)\n",
    "\n",
    "    def import_model(self, filename):\n",
    "        model_data = np.load(filename, allow_pickle=True)\n",
    "        self.weights = model_data['weights']\n",
    "        self.biases = model_data['biases']\n",
    "        self.layer_sizes = model_data['layer_sizes']\n",
    "        self.activations = model_data['activations']\n",
    "        self.cost_fn = model_data['cost_fn']\n",
    "        self.optimizer = model_data['optimizer']\n",
    "        self.init_method = model_data['init_method']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from app import MLP\n",
    "import matplotlib\n",
    "from mnist1d.data import make_dataset, get_dataset_args\n",
    "\n",
    "matplotlib.use('TkAgg')\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, layer_sizes, activations, cost_fn, optimizer, init_method, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.activations = activations\n",
    "        self.cost_fn = cost_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.init_method = init_method\n",
    "        self.beta1 = beta1  # Adam\n",
    "        self.beta2 = beta2  # Adam\n",
    "        self.epsilon = epsilon  # Adam\n",
    "        self.weights, self.biases = self.initialize_weights()\n",
    "        self.m = 0  # number of samples, initialized during training\n",
    "\n",
    "        if optimizer == 'adam':\n",
    "            self.m_weights = [np.zeros_like(w) for w in self.weights]\n",
    "            self.v_weights = [np.zeros_like(w) for w in self.weights]\n",
    "            self.m_biases = [np.zeros_like(b) for b in self.biases]\n",
    "            self.v_biases = [np.zeros_like(b) for b in self.biases]\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        for i in range(1, len(self.layer_sizes)):\n",
    "            if self.init_method == 'xavier':\n",
    "                w = np.random.randn(self.layer_sizes[i], self.layer_sizes[i-1]) * np.sqrt(1 / self.layer_sizes[i-1])\n",
    "            elif self.init_method == 'he':\n",
    "                w = np.random.randn(self.layer_sizes[i], self.layer_sizes[i-1]) * np.sqrt(2 / self.layer_sizes[i-1])\n",
    "            else:  # normal or uniform\n",
    "                w = np.random.randn(self.layer_sizes[i], self.layer_sizes[i-1])\n",
    "            b = np.zeros((self.layer_sizes[i], 1))\n",
    "            weights.append(w)\n",
    "            biases.append(b)\n",
    "        return weights, biases\n",
    "    \n",
    "    def forward(self, X):\n",
    "        activations = [X]\n",
    "        zs = []\n",
    "        for i, (w, b) in enumerate(zip(self.weights, self.biases)):\n",
    "            # Ensure b is broadcasted correctly\n",
    "            z = np.dot(w, activations[-1]) + b  # z = W * a + b\n",
    "            zs.append(z)\n",
    "            if self.activations[i] == 'relu':\n",
    "                activations.append(self.relu(z))\n",
    "            elif self.activations[i] == 'sigmoid':\n",
    "                activations.append(self.sigmoid(z))\n",
    "            elif self.activations[i] == 'tanh':\n",
    "                activations.append(self.tanh(z))\n",
    "            elif self.activations[i] == 'leaky_relu':\n",
    "                activations.append(self.leaky_relu(z))\n",
    "            else:  # Linear activation\n",
    "                activations.append(z)\n",
    "        return activations, zs\n",
    "    \n",
    "    def backward(self, activations, zs, y):\n",
    "        weight_grads = [None] * len(self.weights)\n",
    "        bias_grads = [None] * len(self.biases)\n",
    "        \n",
    "        if self.cost_fn == 'bce':\n",
    "            dz = activations[-1] - y  # for binary cross-entropy\n",
    "        elif self.cost_fn == 'mse':\n",
    "            dz = (activations[-1] - y) * self.deriv_mse(activations[-1], y)\n",
    "        \n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            # dz is (units in current layer, number of samples)\n",
    "            # activations[i] is (units in previous layer, number of samples)\n",
    "            weight_grads[i] = np.dot(dz, activations[i].T) / self.m  # Ensure proper matrix multiplication\n",
    "            bias_grads[i] = np.sum(dz, axis=1, keepdims=True) / self.m  # Sum over the sample dimension\n",
    "            \n",
    "            if i > 0:\n",
    "                dz = np.dot(self.weights[i].T, dz) * self.deriv_activation(zs[i-1], self.activations[i-1])\n",
    "        \n",
    "        return weight_grads, bias_grads\n",
    "    \n",
    "    def train(self, X, y, epochs, batch_size, learning_rate, X_val, y_val):\n",
    "        self.m = X.shape[1]  # number of samples\n",
    "\n",
    "        # Initialize lists to store training loss and validation accuracy\n",
    "        loss_history = []\n",
    "        acc_history = []\n",
    "\n",
    "        # Set up the plot outside the loop with three subplots\n",
    "        plt.ion()  # Turn on interactive mode for live updating plots\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(21, 6))\n",
    "\n",
    "        total_batches = int(np.ceil(self.m / batch_size))  # Total number of batches per epoch\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle the data at the beginning of each epoch\n",
    "            permutation = np.random.permutation(self.m)\n",
    "            X_shuffled = X[:, permutation]\n",
    "            y_shuffled = y[:, permutation]\n",
    "\n",
    "            epoch_loss = 0  # To accumulate loss over the epoch\n",
    "\n",
    "            for batch in range(total_batches):\n",
    "                # Define the start and end indices of the batch\n",
    "                start = batch * batch_size\n",
    "                end = min(start + batch_size, self.m)\n",
    "\n",
    "                # Get the mini-batch data\n",
    "                X_batch = X_shuffled[:, start:end]\n",
    "                y_batch = y_shuffled[:, start:end]\n",
    "\n",
    "                # Forward pass on the mini-batch\n",
    "                activations, zs = self.forward(X_batch)\n",
    "                loss = self.compute_cost(activations[-1], y_batch)\n",
    "                epoch_loss += loss  # Accumulate the loss\n",
    "\n",
    "                # Backward pass\n",
    "                weight_grads, bias_grads = self.backward(activations, zs, y_batch)\n",
    "\n",
    "                # Update parameters\n",
    "                self.update_parameters(weight_grads, bias_grads, learning_rate, epoch)\n",
    "\n",
    "                # print weight hist, loss and acc for each mini_batch\n",
    "                self.update_plots(ax1, ax2, ax3, epoch, loss_history, acc_history, epochs)\n",
    "\n",
    "\n",
    "            # Average loss over the epoch\n",
    "            avg_epoch_loss = epoch_loss / total_batches\n",
    "            loss_history.append(avg_epoch_loss)\n",
    "\n",
    "            # Calculate validation accuracy at the end of the epoch\n",
    "            acc = self.calculate_acc(X_val, y_val)\n",
    "            acc_history.append(acc)\n",
    "\n",
    "            # # Update plots every few epochs\n",
    "            # if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "            #     print(f\"Epoch {epoch}, Loss: {avg_epoch_loss}, Validation Accuracy: {acc}\")\n",
    "            #     self.update_plots(ax1, ax2, ax3, epoch, loss_history, acc_history, epochs)\n",
    "\n",
    "        # Update plots one final time after training\n",
    "        self.update_plots(ax1, ax2, ax3, epoch, loss_history, acc_history, epochs)\n",
    "\n",
    "        plt.ioff()  # Turn off interactive mode\n",
    "        plt.show()  # Show the final plot\n",
    "\n",
    "    def update_plots(self, ax1, ax2, ax3, epoch, loss_history, acc_history, total_epochs):\n",
    "        # Clear the previous plots\n",
    "        ax1.clear()\n",
    "        ax2.clear()\n",
    "        ax3.clear()\n",
    "\n",
    "        # --- Update Weight Histogram on ax1 ---\n",
    "        # Combine all layer weights into a single array for the histogram\n",
    "        all_weights = np.concatenate([w.flatten() for w in self.weights])\n",
    "\n",
    "        # Plot histogram of all weights combined\n",
    "        ax1.hist(all_weights, bins=25, color='blue', alpha=0.7)\n",
    "\n",
    "        # Add title and labels\n",
    "        ax1.set_title(f\"Weight Distribution at Epoch {epoch}\")\n",
    "        ax1.set_xlabel(\"Weight values\")\n",
    "        ax1.set_ylabel(\"Frequency\")\n",
    "\n",
    "        # --- Plot Loss over Epochs on ax2 ---\n",
    "        ax2.plot(loss_history, color='red')\n",
    "\n",
    "        # Add title and labels\n",
    "        ax2.set_title(\"Loss over Epochs\")\n",
    "        ax2.set_xlabel(\"Epoch\")\n",
    "        ax2.set_ylabel(\"Loss\")\n",
    "\n",
    "        ax2.set_xlim(0, total_epochs)  # Set x-axis limits to number of epochs\n",
    "\n",
    "        # --- Plot accuracy over Epochs on ax2 ---\n",
    "        ax3.plot(acc_history, color='red')\n",
    "\n",
    "        # Add title and labels\n",
    "        ax3.set_title(\"Accuracy over Epochs in the Validation Data\")\n",
    "        ax3.set_xlabel(\"Epoch\")\n",
    "        ax3.set_ylabel(\"Accuracy\")\n",
    "\n",
    "        ax3.set_xlim(0, total_epochs)  # Set x-axis limits to number of epochs\n",
    "        ax3.set_ylim(0, 1)  # Set y-axis limits to 0-1\n",
    "\n",
    "        # Adjust layout to prevent overlap\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Draw the updated plots\n",
    "        fig = ax1.figure  # Get the figure object associated with the axes\n",
    "        fig.canvas.draw()\n",
    "        fig.canvas.flush_events()\n",
    "\n",
    "        # Optional: Pause to allow the plot to update\n",
    "        # plt.pause(0.01)\n",
    "\n",
    "    def calculate_acc(self, X_val, y_val):\n",
    "\n",
    "        activations, _ = self.forward(X_val)\n",
    "        y_pred = activations[-1]\n",
    "\n",
    "        # Convert predictions to class labels\n",
    "        y_pred_labels = np.argmax(y_pred, axis=0)\n",
    "        y_true_labels = np.argmax(y_val, axis=0)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        acc = np.mean(y_pred_labels == y_true_labels)\n",
    "        return acc\n",
    "\n",
    "    def update_parameters(self, weight_grads, bias_grads, learning_rate, t):\n",
    "        if self.optimizer == 'sgd':\n",
    "            for i in range(len(self.weights)):\n",
    "                self.weights[i] -= learning_rate * weight_grads[i]\n",
    "                self.biases[i] -= learning_rate * bias_grads[i]\n",
    "        elif self.optimizer == 'adam':\n",
    "            for i in range(len(self.weights)):\n",
    "                # Adam updates\n",
    "                self.m_weights[i] = self.beta1 * self.m_weights[i] + (1 - self.beta1) * weight_grads[i]\n",
    "                self.v_weights[i] = self.beta2 * self.v_weights[i] + (1 - self.beta2) * (weight_grads[i] ** 2)\n",
    "\n",
    "                m_hat_w = self.m_weights[i] / (1 - self.beta1 ** (t + 1))\n",
    "                v_hat_w = self.v_weights[i] / (1 - self.beta2 ** (t + 1))\n",
    "\n",
    "                self.weights[i] -= learning_rate * m_hat_w / (np.sqrt(v_hat_w) + self.epsilon)\n",
    "\n",
    "                self.m_biases[i] = self.beta1 * self.m_biases[i] + (1 - self.beta1) * bias_grads[i]\n",
    "                self.v_biases[i] = self.beta2 * self.v_biases[i] + (1 - self.beta2) * (bias_grads[i] ** 2)\n",
    "\n",
    "                m_hat_b = self.m_biases[i] / (1 - self.beta1 ** (t + 1))\n",
    "                v_hat_b = self.v_biases[i] / (1 - self.beta2 ** (t + 1))\n",
    "\n",
    "                self.biases[i] -= learning_rate * m_hat_b / (np.sqrt(v_hat_b) + self.epsilon)\n",
    "\n",
    "    # Activation functions and their derivatives\n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def tanh(self, z):\n",
    "        return np.tanh(z)\n",
    "    \n",
    "    def leaky_relu(self, z, alpha=0.01):\n",
    "        return np.where(z > 0, z, alpha * z)\n",
    "    \n",
    "    def deriv_activation(self, z, activation_fn):\n",
    "        if activation_fn == 'relu':\n",
    "            return np.where(z > 0, 1, 0)\n",
    "        elif activation_fn == 'sigmoid':\n",
    "            a = self.sigmoid(z)\n",
    "            return a * (1 - a)\n",
    "        elif activation_fn == 'tanh':\n",
    "            return 1 - np.tanh(z)**2\n",
    "        elif activation_fn == 'leaky_relu':\n",
    "            return np.where(z > 0, 1, 0.01)\n",
    "        else:\n",
    "            return 1\n",
    "        \n",
    "        # Add softmax function\n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))  # Subtract max for numerical stability\n",
    "        return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n",
    "    \n",
    "    # Cost functions\n",
    "    def binary_cross_entropy(self, y_pred, y_true):\n",
    "        epsilon = 1e-12  # Small value to avoid log(0)\n",
    "        y_pred = np.clip(y_pred, epsilon, 1. - epsilon)  # Clip predictions to prevent log(0)\n",
    "        m = y_true.shape[1]\n",
    "        cost = -np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)) / m\n",
    "        return np.squeeze(cost)\n",
    "    \n",
    "    def mean_squared_error(self, y_pred, y_true):\n",
    "        # Ensure y_pred and y_true are numpy arrays (not numpy matrices)\n",
    "        y_pred = np.asarray(y_pred)\n",
    "        y_true = np.asarray(y_true)\n",
    "        \n",
    "        return np.mean((y_pred - y_true) ** 2)\n",
    "    \n",
    "    def compute_cost(self, y_pred, y_true):\n",
    "        if self.cost_fn == 'bce':\n",
    "            return self.binary_cross_entropy(y_pred, y_true)\n",
    "        elif self.cost_fn == 'mse':\n",
    "            return self.mean_squared_error(y_pred, y_true)\n",
    "    \n",
    "    def deriv_mse(self, y_pred, y_true):\n",
    "        return 2 * (y_pred - y_true)\n",
    "    \n",
    "    # Model export and import\n",
    "    def export_model(self, filename):\n",
    "        model_data = {\n",
    "            'weights': self.weights,\n",
    "            'biases': self.biases,\n",
    "            'layer_sizes': self.layer_sizes,\n",
    "            'activations': self.activations,\n",
    "            'cost_fn': self.cost_fn,\n",
    "            'optimizer': self.optimizer,\n",
    "            'init_method': self.init_method\n",
    "        }\n",
    "        np.savez(filename, **model_data)\n",
    "\n",
    "    def import_model(self, filename):\n",
    "        model_data = np.load(filename, allow_pickle=True)\n",
    "        self.weights = model_data['weights']\n",
    "        self.biases = model_data['biases']\n",
    "        self.layer_sizes = model_data['layer_sizes']\n",
    "        self.activations = model_data['activations']\n",
    "        self.cost_fn = model_data['cost_fn']\n",
    "        self.optimizer = model_data['optimizer']\n",
    "        self.init_method = model_data['init_method']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data  # Shape: (150 samples, 4 features)\n",
    "y = iris.target.reshape(-1, 1)  # Shape: (150 samples, 1)\n",
    "\n",
    "# One-hot encode the labels\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y = encoder.fit_transform(y)  # Shape: (150 samples, 3 classes)\n",
    "\n",
    "# Normalize the dataset\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)  # Shape: (150 samples, 4 features)\n",
    "\n",
    "# Split into training and val sets (no transposition needed here)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Transpose X_train and y_train for the MLP (MLP expects (features, samples) shape)\n",
    "X_train, X_val = X_train.T, X_val.T  # Now shape is (features, samples)\n",
    "y_train, y_val = y_train.T, y_val.T  # Now shape is (classes, samples)\n",
    "\n",
    "# Initialize the MLP model\n",
    "mlp = MLP(\n",
    "    layer_sizes=[X_train.shape[0], 10, 3],  # 4 input features, 10 units in hidden layer, 3 output classes\n",
    "    activations=['relu', 'softmax'],        # Hidden: ReLU, Output: Softmax for multi-class classification\n",
    "    cost_fn='bce',                          # Use Cross-Entropy Loss\n",
    "    optimizer='sgd',                       # Use Adam optimizer\n",
    "    init_method='xavier'\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "mlp.train(X_train, y_train, epochs=100, batch_size=25, learning_rate=0.01, X_val=X_val, y_val=y_val)\n",
    "\n",
    "# val the model (Forward pass for prediction)\n",
    "activations, _ = mlp.forward(X_val)\n",
    "y_pred = activations[-1]\n",
    "\n",
    "# Convert predictions to class labels\n",
    "y_pred_labels = np.argmax(y_pred, axis=0)\n",
    "y_true_labels = np.argmax(y_val, axis=0)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(y_pred_labels == y_true_labels)\n",
    "\n",
    "# Assuming you have y_pred_labels and y_true_labels already computed\n",
    "conf_matrix = confusion_matrix(y_true_labels, y_pred_labels)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function make_dataset in module mnist1d.data:\n",
      "\n",
      "make_dataset(args=None, template=None)\n",
      "    # make a dataset\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mnist1d.data import make_dataset, get_dataset_args\n",
    "\n",
    "defaults = get_dataset_args()\n",
    "data = make_dataset(defaults)\n",
    "x, y, t = data['x'], data['y'], data['t']\n",
    "\n",
    "help(make_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_profundo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
