{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import time\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "class Activation:\n",
    "    @staticmethod\n",
    "    def relu(z):\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu_derivative(z):\n",
    "        return np.where(z > 0, 1, 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(z):\n",
    "        s = Activation.sigmoid(z)\n",
    "        return s * (1 - s)\n",
    "\n",
    "    @staticmethod\n",
    "    def tanh(z):\n",
    "        return np.tanh(z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh_derivative(z):\n",
    "        return 1 - np.tanh(z)**2\n",
    "\n",
    "    @staticmethod\n",
    "    def LeakyReLU(z, alpha=0.01):\n",
    "        return np.maximum(alpha*z, z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def LeakyReLU_derivative(z, alpha=0.01):\n",
    "        return np.where(z > 0, 1, alpha)\n",
    "\n",
    "    @staticmethod\n",
    "    def linear(z):\n",
    "        return z\n",
    "    \n",
    "    @staticmethod\n",
    "    def linear_derivative(z):\n",
    "        return np.ones_like(z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(z):\n",
    "        # Ficou brabo, not stable \n",
    "        # (https://stackoverflow.com/questions/40575841/numpy-calculate-the-derivative-of-the-softmax-function)\n",
    "        # So we subtract the maximum value from the input to make it stable\n",
    "        e_x = np.exp(z - np.max(z, axis=-1, keepdims=True))\n",
    "        return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax_derivative(z):\n",
    "        # Compute softmax for z\n",
    "        s = Activation.softmax(z)  # shape (batch_size, num_classes)\n",
    "        \n",
    "        # For each sample in the batch, compute the derivative\n",
    "        # Softmax derivative simplification for backpropagation\n",
    "        return s * (1 - s)\n",
    "    \n",
    "class Loss:\n",
    "    @staticmethod\n",
    "    def MSE(y, y_hat):\n",
    "        return 1 / y.shape[0]*(y - y_hat).T @ (y - y_hat)\n",
    "    \n",
    "    def MSE_derivative(y, y_hat):\n",
    "        return y_hat - y\n",
    "\n",
    "    @staticmethod\n",
    "    def BCE(y, y_hat):\n",
    "        return -1 / y.shape[0] * (y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
    "    \n",
    "    def BCE_derivative(y, y_hat):\n",
    "        return (y_hat - y) / (y_hat * (1 - y_hat))\n",
    "    \n",
    "    @staticmethod\n",
    "    def CrossEntropy(y, y_hat):\n",
    "        y_hat = np.clip(y_hat, 1e-15, 1-1e-15)\n",
    "        return -1 / y.shape[0] * np.sum(y * np.log(y_hat))\n",
    "    \n",
    "    def CrossEntropy_derivative(y, y_hat):\n",
    "        return y_hat - y\n",
    "    \n",
    "class Initialization:\n",
    "    @staticmethod\n",
    "    def xavier(size):\n",
    "        (fan_in, fan_out) = size[0], size[1]\n",
    "        stddev = np.sqrt(2 / (fan_in + fan_out))\n",
    "        return np.random.normal(0, stddev, size=size)\n",
    "    \n",
    "    @staticmethod\n",
    "    def he(size):\n",
    "        fan_in = size[0]\n",
    "        stddev = np.sqrt(2 / fan_in)\n",
    "        return np.random.normal(0, stddev, size=size)\n",
    "    \n",
    "    @staticmethod\n",
    "    def normal(size):\n",
    "        return np.random.randn(*size)\n",
    "    \n",
    "    @staticmethod\n",
    "    def uniform(size):\n",
    "        return np.random.uniform(size=size)\n",
    "    \n",
    "    @staticmethod\n",
    "    def zeros(size):\n",
    "        return np.zeros(size)\n",
    "    \n",
    "    @staticmethod\n",
    "    def explode(size):\n",
    "        return 10 * np.random.exponential(size=size)\n",
    "\n",
    "    @staticmethod\n",
    "    def vanish(size):\n",
    "        return 1 / 100 * np.random.exponential(size=size)\n",
    "\n",
    "class Optimizer:\n",
    "    def __init__(self, learning_rate, network):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.grads = network.grads\n",
    "        self.weights = network.weights\n",
    "        self.biases = network.biases\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        return {key: np.zeros_like(value) for key, value in self.grads.items()}\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, learning_rate, network):\n",
    "        super().__init__(learning_rate, network)\n",
    "        \n",
    "    def step(self):\n",
    "        for i in range(len(self.weights)):\n",
    "            \n",
    "            self.weights[i] = self.weights[i] - self.learning_rate * self.grads[f'dw{i+1}']\n",
    "            self.biases[i] = self.biases[i] - self.learning_rate * self.grads[f'db{i+1}']\n",
    "            \n",
    "        return self.weights, self.biases\n",
    "\n",
    "class Adam(Optimizer):\n",
    "    def __init__(self, learning_rate, network, beta1=0.9, gamma=0.999, epsilon=1e-8):\n",
    "        super().__init__(learning_rate, network)\n",
    "        self.beta1 = beta1\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        # Initializing as zeros so that we can have momentum and RMSprop for each parameter\n",
    "        self.m_w = [np.zeros_like(w) for w in self.weights]\n",
    "        self.v_w = [np.zeros_like(w) for w in self.weights]\n",
    "        self.m_b = [np.zeros_like(b) for b in self.biases]\n",
    "        self.v_b = [np.zeros_like(b) for b in self.biases]\n",
    "        self.t = 0\n",
    "        \n",
    "    def step(self):\n",
    "        self.t += 1\n",
    "        for i in range(len(self.weights)):\n",
    "            # Update for weights\n",
    "            self.m_w[i] = self.beta1 * self.m_w[i] + (1 - self.beta1) * self.grads[f'dw{i+1}']\n",
    "            self.v_w[i] = self.gamma * self.v_w[i] + (1 - self.gamma) * np.square(self.grads[f'dw{i+1}'])\n",
    "            m_w_hat = self.m_w[i] / (1 - self.beta1**self.t)\n",
    "            v_w_hat = self.v_w[i] / (1 - self.gamma**self.t)\n",
    "            self.weights[i] = self.weights[i] - self.learning_rate * m_w_hat / (np.sqrt(v_w_hat) + self.epsilon)\n",
    "            \n",
    "            # Update for biases\n",
    "            self.m_b[i] = self.beta1 * self.m_b[i] + (1 - self.beta1) * self.grads[f'db{i+1}']\n",
    "            self.v_b[i] = self.gamma * self.v_b[i] + (1 - self.gamma) * np.square(self.grads[f'db{i+1}'])\n",
    "            m_b_hat = self.m_b[i] / (1 - self.beta1**self.t)\n",
    "            v_b_hat = self.v_b[i] / (1 - self.gamma**self.t)\n",
    "            self.biases[i] = self.biases[i] - self.learning_rate * m_b_hat / (np.sqrt(v_b_hat) + self.epsilon)\n",
    "        \n",
    "        return self.weights, self.biases\n",
    "    \n",
    "    \n",
    "class MLP:\n",
    "    def __init__(self, neurons, activations, weight_initialization) -> None:\n",
    "        self.MLP_DEPTH = len(neurons) - 1\n",
    "        self.neurons = neurons\n",
    "        if len(activations) == 1:\n",
    "            self.activations = [activations[0] for _ in range(self.MLP_DEPTH)]\n",
    "        elif len(activations) == 2 and self.MLP_DEPTH > 2:\n",
    "            self.activations = [activations[0] for _ in range(self.MLP_DEPTH)]\n",
    "            self.activations[-1] = activations[1]\n",
    "        else:\n",
    "            self.activations = activations\n",
    "            \n",
    "        self.activation_derivatives = [getattr(Activation, f\"{activation.__name__}_derivative\") for activation in self.activations]\n",
    "        \n",
    "        \n",
    "        if isinstance(weight_initialization, list):\n",
    "            # Direct weight assignment\n",
    "            self.weights = weight_initialization\n",
    "        else:\n",
    "            # Use initialization method\n",
    "            self.weights = [np.squeeze(weight_initialization((neurons[i+1], neurons[i]))) for i in range(self.MLP_DEPTH)]\n",
    "            \n",
    "        self.biases = [np.zeros((neurons[i + 1])) for i in range(self.MLP_DEPTH)]\n",
    "        \n",
    "        self.cache = {}\n",
    "        self.grads = {}\n",
    "    \n",
    "    def forward(self, input):\n",
    "\n",
    "        self.cache['a0'] = a = input\n",
    "        \n",
    "        for i in range(self.MLP_DEPTH):\n",
    "            z = a @ self.weights[i].T + self.biases[i]\n",
    "            a = self.activations[i](z)\n",
    "\n",
    "            self.cache[f'w{i+1}'] = self.weights[i]\n",
    "            self.cache[f'z{i+1}'] = z\n",
    "            self.cache[f'a{i+1}'] = a\n",
    "            \n",
    "        return a\n",
    "\n",
    "    def backward(self, y, loss):\n",
    "        BATCH_SIZE = y.shape[0]\n",
    "        \n",
    "        self.loss_derivative = getattr(Loss, f\"{loss.__name__}_derivative\")\n",
    "        \n",
    "        # Output layer\n",
    "        dz = self.loss_derivative(y, self.cache[f'a{self.MLP_DEPTH}'])\n",
    "        da = self.activation_derivatives[self.MLP_DEPTH-1](self.cache[f'z{self.MLP_DEPTH}'])\n",
    "        dl = dz * da\n",
    "        \n",
    "        self.grads[f'dw{self.MLP_DEPTH}'] = (1/BATCH_SIZE) * dl.T @ self.cache[f'a{self.MLP_DEPTH-1}']\n",
    "        self.grads[f'db{self.MLP_DEPTH}'] = (1/BATCH_SIZE) * np.sum(dz, axis=0, keepdims=True)\n",
    "\n",
    "        for i in range(self.MLP_DEPTH, 1, -1):\n",
    "            if len(dl.shape) == 1:\n",
    "                dl = dl.reshape(-1, 1)\n",
    "            if len(self.cache[f'w{i}'].shape) == 1:\n",
    "                self.cache[f'w{i}'] = self.cache[f'w{i}'].reshape(1, -1)\n",
    "\n",
    "            dz = dl @ self.cache[f'w{i}']\n",
    "\n",
    "            dz *= self.activation_derivatives[i-1](self.cache[f'a{i-1}'])\n",
    "            \n",
    "            self.grads[f'dw{i-1}'] = (1/BATCH_SIZE) * dz.T @ self.cache[f'a{i-2}']\n",
    "            self.grads[f'db{i-1}'] = (1/BATCH_SIZE) * np.sum(dz, axis=0, keepdims=True)\n",
    "            dl = dz\n",
    "            \n",
    "            \n",
    "    def save(self, filepath):\n",
    "        \"\"\"Save the model weights and biases to a file.\"\"\"\n",
    "        model_data = {\n",
    "            'weights': self.weights,\n",
    "            'biases': self.biases,\n",
    "            'neurons': self.neurons,\n",
    "            'activations': [act.__name__ for act in self.activations]\n",
    "        }\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "        print(f\"Model saved to {filepath}\")\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, filepath, weight_initialization):\n",
    "        \"\"\"Load a model from a file.\"\"\"\n",
    "        with open(filepath, 'rb') as f:\n",
    "            model_data = pickle.load(f)\n",
    "        \n",
    "        # Recreate activation functions from their names\n",
    "        activations = [getattr(Activation, act_name) for act_name in model_data['activations']]\n",
    "        \n",
    "        # Create a new instance of the model\n",
    "        model = cls(model_data['neurons'], activations, weight_initialization)\n",
    "        \n",
    "        # Load the weights and biases\n",
    "        model.weights = model_data['weights']\n",
    "        model.biases = model_data['biases']\n",
    "        \n",
    "        print(f\"Model loaded from {filepath}\")\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, train_set, train_labels, test_set, test_labels, val_split=0.15, num_classes=10):\n",
    "        self.train_set = train_set\n",
    "        self.test_set = test_set\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # One-hot encode the labels\n",
    "        self.train_labels = self.one_hot_encode(train_labels)\n",
    "        self.test_labels = self.one_hot_encode(test_labels)\n",
    "        \n",
    "        # Create validation set\n",
    "        val_size = int(len(train_set) * val_split)\n",
    "        self.val_set = train_set[-val_size:]\n",
    "        self.val_labels = self.train_labels[-val_size:]\n",
    "        self.train_set = train_set[:-val_size]\n",
    "        self.train_labels = self.train_labels[:-val_size]\n",
    "    \n",
    "    def one_hot_encode(self, labels):\n",
    "        # Ensure labels is a 1D array\n",
    "        labels = np.array(labels).ravel()\n",
    "        # Check if labels are in the correct range\n",
    "        if np.min(labels) < 0 or np.max(labels) >= self.num_classes:\n",
    "            raise ValueError(f\"Labels must be in the range 0 to {self.num_classes-1}\")\n",
    "        # Create one-hot encoded array\n",
    "        one_hot = np.zeros((labels.size, self.num_classes))\n",
    "        one_hot[np.arange(labels.size), labels] = 1\n",
    "        return one_hot\n",
    "    \n",
    "    def create_batches(self, batch_size, dataset='train'):\n",
    "        if dataset == 'train':\n",
    "            data, labels = self.train_set, self.train_labels\n",
    "        elif dataset == 'val':\n",
    "            data, labels = self.val_set, self.val_labels\n",
    "        elif dataset == 'test':\n",
    "            data, labels = self.test_set, self.test_labels\n",
    "        else:\n",
    "            raise ValueError(\"dataset must be 'train', 'val', or 'test'\")\n",
    "        \n",
    "        indices = np.arange(len(data))\n",
    "        np.random.shuffle(indices)\n",
    "        for start_idx in range(0, len(data), batch_size):\n",
    "            batch_indices = indices[start_idx:start_idx+batch_size]\n",
    "            yield data[batch_indices], labels[batch_indices]\n",
    "    \n",
    "    def get_test_data(self):\n",
    "        return self.test_set, self.test_labels\n",
    "    \n",
    "    def get_val_data(self):\n",
    "        return self.val_set, self.val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_plots_grad_actv(ax1, ax2, weight_grads, activations):\n",
    "        # Clear the previous plots\n",
    "        ax1.clear()\n",
    "        ax2.clear()\n",
    "\n",
    "        # --- Update Weight Histogram on ax1 ---\n",
    "        # Combine all layer weights into a single array for the histogram\n",
    "        # print(len(activations))\n",
    "        all_grads = np.concatenate([w.flatten() for w in weight_grads[0]])\n",
    "        all_activations = np.concatenate([a.flatten() for a in activations[0]])\n",
    "\n",
    "        # Plot histogram of all weights combined\n",
    "        ax1.hist(all_grads, bins=25, color='blue', alpha=0.7)\n",
    "\n",
    "        # Add title and labels\n",
    "        ax1.set_title(f\"Weight Grads Distribution\")\n",
    "        ax1.set_xlabel(\"Weight values\")\n",
    "        ax1.set_ylabel(\"Frequency\")\n",
    "\n",
    "        # --- Plot Loss over Epochs on ax2 ---\n",
    "        ax2.hist(all_activations, bins=25, color='red', alpha=0.7)\n",
    "\n",
    "        # Add title and labels\n",
    "        ax2.set_title(\"Activations Distribution\")\n",
    "        ax2.set_xlabel(\"Activation values\")\n",
    "        ax2.set_ylabel(\"Frequency\")\n",
    "\n",
    "        # Adjust layout to prevent overlap\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Draw the updated plots\n",
    "        fig = ax1.figure  # Get the figure object associated with the axes\n",
    "        fig.canvas.draw()\n",
    "        fig.canvas.flush_events()\n",
    "\n",
    "def update_plots_loss_acc(bx1, bx2, epoch, loss_history, acc_history, total_epochs):\n",
    "        # Clear the previous plots\n",
    "        bx1.clear()\n",
    "        bx2.clear()\n",
    "\n",
    "        # --- Plot Loss over Epochs on bx2 ---\n",
    "        bx1.plot(loss_history, color='red')\n",
    "\n",
    "        # Add title and labels\n",
    "        bx1.set_title(\"Loss over Epochs\")\n",
    "        bx1.set_xlabel(\"Epoch\")\n",
    "        bx1.set_ylabel(\"Loss\")\n",
    "\n",
    "        bx1.set_xlim(0, total_epochs)  # Set x-axis limits to number of epochs\n",
    "\n",
    "        # --- Plot accuracy over Epochs on ax2 ---\n",
    "        bx2.plot(acc_history, color='red')\n",
    "\n",
    "        # Add title and labels\n",
    "        bx2.set_title(\"Accuracy over Epochs in the Validation Data\")\n",
    "        bx2.set_xlabel(\"Epoch\")\n",
    "        bx2.set_ylabel(\"Accuracy\")\n",
    "\n",
    "        bx2.set_xlim(0, total_epochs)  # Set x-axis limits to number of epochs\n",
    "        bx2.set_ylim(0, 1)  # Set y-axis limits to 0-1\n",
    "\n",
    "        # Adjust layout to prevent overlap\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Draw the updated plots\n",
    "        fig = bx1.figure  # Get the figure object associated with the axes\n",
    "        fig.canvas.draw()\n",
    "        fig.canvas.flush_events()\n",
    "\n",
    "        # Optional: Pause to allow the plot to update\n",
    "        # plt.pause(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_whole_set(model, test_data, test_labels, loss_fn):\n",
    "    y_hat = model.forward(test_data)\n",
    "    total_loss = loss_fn(test_labels, y_hat)\n",
    "    \n",
    "    predictions = np.argmax(y_hat, axis=1)\n",
    "    true_labels = np.argmax(test_labels, axis=1)\n",
    "    accuracy = np.mean(predictions == true_labels)\n",
    "    \n",
    "    return accuracy, total_loss\n",
    "\n",
    "\n",
    "def train(model, dataset, epochs, optimizer, loss_fn, batch_size=32, save_path=\"./models/best_model.pkl\"):\n",
    "    metrics = {}\n",
    "    metrics[\"train_loss\"] = []\n",
    "    metrics[\"val_loss\"] = []\n",
    "    metrics[\"val_acc\"] = []\n",
    "    metrics[\"network_grads\"] = []\n",
    "    metrics[\"network_activations\"] = []\n",
    "    metrics[\"train_time\"] = []\n",
    "    best_val_acc = 0\n",
    "    \n",
    "    # Set up the plot outside the loop with three subplots\n",
    "    plt.ion()  # Turn on interactive mode for live updating plots\n",
    "    fig1, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    fig2, (bx3, bx4) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        batch_loss = 0\n",
    "        \n",
    "        for X_batch, y_batch in dataset.create_batches(batch_size, 'train'):\n",
    "            \n",
    "            start = time.time()\n",
    "            pred = model.forward(X_batch)\n",
    "            batch_loss += loss_fn(y_batch, pred)\n",
    "            model.backward(y_batch, loss_fn)\n",
    "            model.weights, model.biases = optimizer.step()\n",
    "            metrics[\"network_grads\"].append([v for v in model.grads.values()])\n",
    "            metrics[\"network_activations\"].append([v for k,v in model.cache.items() if 'a' in k and '0' not in k])\n",
    "            update_plots_grad_actv(ax1, ax2, metrics[\"network_grads\"][-1], metrics[\"network_activations\"][-1])\n",
    "            \n",
    "        metrics[\"train_time\"].append(time.time() - start)\n",
    "        \n",
    "        # Validate on validation set\n",
    "        val_acc, val_loss = test_whole_set(model, *dataset.get_val_data(), loss_fn)\n",
    "        metrics[\"val_loss\"].append(val_loss)\n",
    "        metrics[\"val_acc\"].append(val_acc)\n",
    "        metrics[\"train_loss\"].append(batch_loss / (len(dataset.train_set) // batch_size))\n",
    "        update_plots_loss_acc(bx3, bx4, epoch, metrics[\"train_loss\"], metrics[\"val_acc\"], epochs)\n",
    "        # print(f\"Epoch {epoch} Train Loss: {metrics['train_loss'][-1]:.4f}, Val Acc: {val_acc:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Save the best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            model.save(save_path)\n",
    "            print(f\"New best model saved with validation accuracy: {best_val_acc:.4f}\")\n",
    "            \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = np.load('data/mnist_train_data.npy')\n",
    "train_labels = np.load('data/mnist_train_labels.npy')\n",
    "test_set = np.load('data/mnist_test_data.npy')\n",
    "test_labels = np.load('data/mnist_test_labels.npy')\n",
    "\n",
    "dataset = Dataset(train_set, train_labels, test_set, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZE = 64\n",
    "loss_fn = Loss.CrossEntropy\n",
    "MODEL = MLP([784, 512, 10], [Activation.relu, Activation.softmax], Initialization.he)\n",
    "optim = Adam(1e-4, MODEL)\n",
    "training_metrics  = train(MODEL, dataset, EPOCHS, optim, loss_fn, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Later, you can load the best model\n",
    "loaded_model = MLP.load('models/best_model.pkl', Initialization.he)\n",
    "\n",
    "# Test the loaded model\n",
    "test_acc, test_loss = test_whole_set(loaded_model, *dataset.get_test_data(), loss_fn)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}, Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.argmax(MODEL.forward(test_set[0])), test_labels[0])\n",
    "plt.plot(training_metrics[\"train_loss\"])\n",
    "plt.plot(training_metrics[\"val_loss\"])\n",
    "plt.show()\n",
    "plt.plot(training_metrics[\"val_acc\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0.5, .1], [.2, .6]])\n",
    "y = np.array([.7, .8])\n",
    "w0 = np.array([[.5, .2], [.6, -.1], [-.4, -.3]])\n",
    "w1 = np.array([.7, -.1, .2])\n",
    "\n",
    "network = MLP([2, 3, 1], [Activation.sigmoid], [w0,w1])\n",
    "\n",
    "output = network.forward(X)\n",
    "loss = Loss.MSE(y, output)\n",
    "print(output)\n",
    "print(loss)\n",
    "\n",
    "network.backward(y, Loss.MSE)\n",
    "optim = SGD(0.1, network)\n",
    "network.weights, network.biases = optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Solve MNIST 1D\n",
    "# TODO: Plotting Activ and grads in mini-batches\n",
    "# TODO: Plot val/train loss --\n",
    "# TODO: Plotting acc / epoch --"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "backprop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
